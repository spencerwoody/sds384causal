
                  

%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Galyardt at 2014-08-21 22:49:39 -0400 

@article{gerber2008social,
  title={Social pressure and voter turnout: Evidence from a
                  large-scale field experiment},
  author={Gerber, Alan S and Green, Donald P and Larimer, Christopher W},
  journal={American political Science review},
  volume={102},
  number={1},
  pages={33--48},
  year={2008},
  publisher={Cambridge University Press}
}
                  

@article{rosenbaum1987,
    author = {Rosenbaum, Paul R.},
    title = "{Sensitivity analysis for certain permutation inferences
                  in matched observational studies}",
    journal = {Biometrika},
    volume = 74,
    number = 1,
    pages = {13-26},
    year = 1987,
    month = 03,
    abstract = "{In observational studies, treatments are not randomly
                  assigned to experimental units, so that
                  randomization tests and their associated interval
                  estimates are not generally applicable. In an effort
                  to compensate for the lack of randomization, treated
                  and control units are often matched on the basis of
                  observed covariates; however, the possibility
                  remains of bias due to residual imbalances in
                  unobserved covariates. A general though simple
                  method is proposed for displaying the sensitivity of
                  permutation inferences to a range of assumptions
                  about unobserved covariates in matched observational
                  studies. The sensitivity analysis is applicable to
                  Wilcoxon's signed rank test, to the McNemar-Cox test
                  for paired binary responses, and to some matching
                  problems with a variable number of controls.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/74.1.13},
    url = {https://doi.org/10.1093/biomet/74.1.13},
    eprint = {https://academic.oup.com/biomet/article-pdf/74/1/13/576656/74-1-13.pdf},
}                  

@article{robins2000marginal,
  title={Marginal structural models and causal inference in epidemiology},
  author={Robins, James M and Hernan, Miguel Angel and Brumback, Babette},
  year={2000},
  journal={Epidemiology}
}
                  

@article{bac,
author = {Wang, Chi and Dominici, Francesca and Parmigiani, Giovanni and Zigler, Corwin Matthew},
title = {Accounting for uncertainty in confounder and effect modifier selection when estimating average causal effects in generalized linear models},
journal = {Biometrics},
volume = {71},
number = {3},
pages = {654-665},
keywords = {Average causal effect, Bayesian adjustment for confounding, Confounder selection, Treatment effect heterogeneity},
doi = {10.1111/biom.12315},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12315},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.12315},
abstract = {Summary Confounder selection and adjustment are essential elements of assessing the causal effect of an exposure or treatment in observational studies. Building upon work by Wang et al. (2012, Biometrics 68, 661–671) and Lefebvre et al. (2014, Statistics in Medicine 33, 2797–2813), we propose and evaluate a Bayesian method to estimate average causal effects in studies with a large number of potential confounders, relatively few observations, likely interactions between confounders and the exposure of interest, and uncertainty on which confounders and interaction terms should be included. Our method is applicable across all exposures and outcomes that can be handled through generalized linear models. In this general setting, estimation of the average causal effect is different from estimation of the exposure coefficient in the outcome model due to noncollapsibility. We implement a Bayesian bootstrap procedure to integrate over the distribution of potential confounders and to estimate the causal effect. Our method permits estimation of both the overall population causal effect and effects in specified subpopulations, providing clear characterization of heterogeneous exposure effects that may vary considerably across different covariate profiles. Simulation studies demonstrate that the proposed method performs well in small sample size situations with 100–150 observations and 50 covariates. The method is applied to data on 15,060 US Medicare beneficiaries diagnosed with a malignant brain tumor between 2000 and 2009 to evaluate whether surgery reduces hospital readmissions within 30 days of diagnosis.},
year = {2015}
}
                  

                  @article{yang2016,
author = {Yang, Shu and Imbens, Guido W. and Cui, Zhanglin and Faries, Douglas E. and Kadziola, Zbigniew},
title = {Propensity score matching and subclassification in observational studies with multi-level treatments},
journal = {Biometrics},
volume = {72},
number = {4},
pages = {1055-1065},
keywords = {Generalized propensity score, Matching, Multi-level treatments, Potential outcomes, Subclassification, Unconfoundedness},
doi = {10.1111/biom.12505},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12505},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.12505},
abstract = {Summary In this article, we develop new methods for estimating average treatment effects in observational studies, in settings with more than two treatment levels, assuming unconfoundedness given pretreatment variables. We emphasize propensity score subclassification and matching methods which have been among the most popular methods in the binary treatment literature. Whereas the literature has suggested that these particular propensity-based methods do not naturally extend to the multi-level treatment case, we show, using the concept of weak unconfoundedness and the notion of the generalized propensity score, that adjusting for a scalar function of the pretreatment variables removes all biases associated with observed pretreatment variables. We apply the proposed methods to an analysis of the effect of treatments for fibromyalgia. We also carry out a simulation study to assess the finite sample performance of the methods relative to previously proposed methods.},
year = {2016}
}




@article{wu2020matching,
    title={Matching on Generalized Propensity Scores with Continuous Exposures},
    author={Xiao Wu and Fabrizia Mealli and Marianthi-Anna Kioumourtzoglou and Francesca Dominici and Danielle Braun},
    year=2020,
    journal={arxiv 1812.06575},
    eprint={1812.06575},
    archivePrefix={arXiv},
    primaryClass={stat.ME}
}

                  
@article{stuart2008,
  title={Using full matching to estimate causal effects in nonexperimental studies: examining the relationship between adolescent marijuana use and adult outcomes.},
  author={Stuart, Elizabeth A and Green, Kerry M},
  journal={Developmental psychology},
  volume={44},
  number={2},
  pages={395},
  year={2008},
  publisher={American Psychological Association}
}
                  

@article{hansen2004,
author = {Ben B Hansen},
title = {Full Matching in an Observational Study of Coaching for the SAT},
journal = {Journal of the American Statistical Association},
volume = {99},
number = {467},
pages = {609-618},
year  = {2004},
publisher = {Taylor & Francis},
doi = {10.1198/016214504000000647},

URL = { 
        https://doi.org/10.1198/016214504000000647
    
},
eprint = { 
        https://doi.org/10.1198/016214504000000647
    
}

}
                  

@article{rosenbaum2001,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2345589},
 abstract = {Subclassification and matching are often used to adjust for covariates in observational studies. Here, the form of an `optimal' subclassification is determined, i.e. a subclassification which makes the treated and control subjects in the same subclass as similar as possible. This optimal form is simple, intuitive and quite practical, though it is rarely used. The optimal form has several consequences. First, it suggests that theoretical investigations may assume that observational studies have this form, for if they do not they may be changed to this form without a loss, and often with a gain. Second, in searching for an optimal subclassification, the search may be confined to subclassifications of this form, for one is sure to be optimal. Finally, it is shown that finding an optimal subclassification of this form may often be reduced to finding the minimum cost flow in a certain network, i.e. to a standard combinatorial optimization problem for which good algorithms exist.},
 author = {Paul R. Rosenbaum},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {3},
 pages = {597--610},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {A Characterization of Optimal Designs for Observational Studies},
 volume = {53},
 year = {1991}
}

@misc{svje2017,
    title={Generalized full matching and extrapolation of the results from a large-scale voter mobilization experiment},
    author={Fredrik S\"avje and Michael J. Higgins and Jasjeet S. Sekhon},
    year={2017},
    eprint={1703.03882},
    archivePrefix={arXiv},
    primaryClass={stat.ME}
}


@article{nattino2019,
author={Giovanni Nattino and Bo Lu and Junxin Shi and Stanley Lemeshow and Henry Xiang},
title={Triplet Matching for Estimating Causal Effects With Three Treatment Arms: A Comparative Study of Mortality by Trauma Center Level},
journal={Journal of the American Statistical Association},
volume={0},
number={0},
pages={1-10},
year ={2020},
publisher={Taylor & Francis},
doi={10.1080/01621459.2020.1737078},

URL={ 
        https://doi.org/10.1080/01621459.2020.1737078
    
},
eprint={ 
        https://doi.org/10.1080/01621459.2020.1737078
    
}

}



                  
%% Saved with string encoding Unicode (UTF-8) 

@article{kennedy2017,
author = {Kennedy, Edward H. and Ma, Zongming and McHugh, Matthew
                  D. and Small, Dylan S.},
title = {Non-parametric methods for doubly robust estimation of
                  continuous treatment effects},
journal = {Journal of the Royal Statistical Society: Series B
                  (Statistical Methodology)},
volume = {79},
number = {4},
pages = {1229-1245},
keywords = {Causal inference, Dose–response, Efficient influence
                  function, Kernel smoothing, Semiparametric
                  estimation},
doi = {10.1111/rssb.12212},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12212},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12212},
abstract = {Summary Continuous treatments (e.g. doses) arise often in
                  practice, but many available causal effect
                  estimators are limited by either requiring
                  parametric models for the effect curve, or by not
                  allowing doubly robust covariate adjustment. We
                  develop a novel kernel smoothing approach that
                  requires only mild smoothness assumptions on the
                  effect curve and still allows for misspecification
                  of either the treatment density or outcome
                  regression. We derive asymptotic properties and give
                  a procedure for data-driven bandwidth selection. The
                  methods are illustrated via simulation and in a
                  study of the effect of nurse staffing on hospital
                  readmissions penalties.},
year = {2017}
}

                  

@article{Imbens2000,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2673642},
 abstract = {Estimation of average treatment effects in observational
                  studies often requires adjustment for differences in
                  pre-treatment variables. If the number of
                  pre-treatment variables is large, standard
                  covariance adjustment methods are often
                  inadequate. Rosenbaum \& Rubin (1983) propose an
                  alternative method for adjusting for pre-treatment
                  variables for the binary treatment case based on the
                  so-called propensity score. Here an extension of the
                  propensity score methodology is proposed that allows
                  for estimation of average casual effects with
                  multi-valued treatments.},
 author = {Guido W. Imbens},
 journal = {Biometrika},
 number = {3},
 pages = {706--710},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The Role of the Propensity Score in Estimating Dose-Response Functions},
 volume = {87},
 year = {2000}
}
                  

@inbook{Hirano2004,
author = {Hirano, Keisuke and Imbens, Guido W.},
publisher = {John Wiley \& Sons, Ltd},
isbn = {9780470090459},
title = {The Propensity Score with Continuous Treatments},
booktitle = {Applied Bayesian Modeling and Causal Inference from Incomplete‐Data Perspectives},
chapter = {7},
pages = {73-84},
doi = {10.1002/0470090456.ch7},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0470090456.ch7},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/0470090456.ch7},
year = {2004},
keywords = {generalized propensity score (GPS), binary treatment case, propensity score methodology, conventional regression estimates, weak unconfoundedness, marginal structural model (MSM), asymptotic standard errors, dose-response function},
abstract = {Summary This chapter contains sections titled: Introduction The basic framework Bias removal using the GPS Estimation and inference Application: the Imbens–Rubin–Sacerdote lottery sample Conclusion}
}
                  

@article{pap2020,
    title={A causal exposure response function with local adjustment for confounding: Estimating health effects of exposure to low levels of ambient fine particulate matter},
    author={Georgia Papadogeorgou and Francesca Dominici},
    year={2018},
    eprint={1806.00928},
    archivePrefix={arXiv},
    journal={arXiv 1806.00928},
    primaryClass={stat.ME}
}

@article{Moodie2012,
author = {Erica EM Moodie and David A Stephens},
title ={Estimation of dose–response functions for longitudinal data using the generalised propensity score},
journal = {Statistical Methods in Medical Research},
volume = {21},
number = {2},
pages = {149-166},
year = {2012},
doi = {10.1177/0962280209340213},
    note ={PMID: 20442194},

URL = { 
        https://doi.org/10.1177/0962280209340213
    
},
eprint = { 
        https://doi.org/10.1177/0962280209340213
    
}
,
    abstract = { In a longitudinal study of dose–response, it is often necessary to adjust for confounding or non-compliance, which may otherwise compromise the estimation of the true effect of a treatment. Using an approach based on the generalised propensity score (GPS) – a generalisation of the classical, binary treatment propensity score – it is possible to construct a balancing score that provides an estimation procedure for the true (unconfounded) direct effect of dose on response. Previously, the GPS has been applied only in a single interval setting; in this article, we extend the GPS methodology to the longitudinal setting to estimate the direct effect of a continuous dose on a longitudinal response. The methodology is applied to two simulated examples, and a real longitudinal dose–response investigation, the Monitored Occlusion Treatment of Amblyopia Study (MOTAS). In the treatment of childhood amblyopia, a common ophthalmological condition, occlusion therapy (patching) was for many decades the standard medical treatment, despite the fact that its efficacy was not quantified. MOTAS was revolutionary, as it was the first study to obtain precise measurements of the amount of occlusion each study participant received over the course of the study. }
}

@article{imai2004,
author = {Kosuke Imai and David A van Dyk},
title = {Causal Inference With General Treatment Regimes},
journal = {Journal of the American Statistical Association},
volume = {99},
number = {467},
pages = {854-866},
year  = {2004},
publisher = {Taylor \& Francis},
doi = {10.1198/016214504000001187},

URL = { 
        https://doi.org/10.1198/016214504000001187
    
},
eprint = { 
        https://doi.org/10.1198/016214504000001187
    
}

}                  

                  

@article{woody2019model,
  title={Model interpretation through lower-dimensional posterior
                  summarization},
  author={Woody, Spencer and Carvalho, Calos M. and Murray, Jared
                  S. },
  journal={arXiv preprint},
  year={2019}
}                  

@Misc{bcf,
    title = {Bayesian regression tree models for causal inference: regularization, confounding, and heterogeneous effects},
    author = {P. Richard Hahn and Jared S. Murray and Carlos M. Carvalho},
    year = {2017},
    eprint = {arXiv:1706.09523},
    url = {https://arxiv.org/abs/1706.09523},
  }                  

@book{berger2013statistical,
  title={Statistical decision theory: foundations, concepts, and methods},
  author={Berger, James},
  year={2013},
  publisher={Springer Science \& Business Media}
}
                  

@ARTICLE{carvalho2019fitting,
       author = {{Carvalho}, Carlos M. and {Hahn}, P. Richard and
                  {McCulloch}, Robert E.},
        title = "{Fitting the fit: variable selection using surrogate
                  models and decision analysis}",
      journal = {Working paper},
         year = "2019",
        month = "May"}
                  

@inproceedings{piironen2016projection,
  title={Projection predictive model selection for Gaussian
                  processes},
  author={Piironen, Juho and Vehtari, Aki},
  booktitle={2016 IEEE 26th International Workshop on Machine Learning
                  for Signal Processing (MLSP)},
  pages={1--6},
  year={2016},
  organization={IEEE}
}
                  

@article{lee2014inference,
  title={Inference functions in high dimensional Bayesian inference},
  author={Lee, Juhee and MacEachern, Steven N},
  journal={Statistics and Its Interface},
  volume={7},
  number={4},
  pages={477--486},
  year={2014},
  publisher={International Press of Boston}
}
                  

@article{giannone2017economic,
  title={Economic predictions with big data: The illusion of
                  sparsity},
  author={Giannone, Domenico and Lenza, Michele and Primiceri, Giorgio
                  E},
  year={2017}
}
                  

@article{crawford2019predictor,
  author = {L. Crawford and S. R. Flaxman and D. E. Runcie and
                  M. West},
  title = {Predictor variable prioritization in non-linear models: A
                  genetic association case study},
  journal = {Annals of Applied Statistics},
  year = {2019},
  volume = {to appear},
  note = {arXiv:1801.07318},
  url =
                  {https://www.e-publications.org/ims/submission/AOAS/user/submissionFile/35543?confirm=7423f152},
  arxiv = {1801.07318}
}

@ARTICLE{ray2018signal,
       author = {{Ray}, Pallavi and {Bhattacharya}, Anirban},
        title = "{Signal Adaptive Variable Selector for the Horseshoe
                  Prior}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Methodology},
         year = "2018",
        month = "Oct",
          eid = {arXiv:1810.09004},
        pages = {arXiv:1810.09004},
archivePrefix = {arXiv},
       eprint = {1810.09004},
 primaryClass = {stat.ME},
       adsurl =
                  {https://ui.adsabs.harvard.edu/abs/2018arXiv181009004R},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
                  

@article{maceachern2019economic,
  title={Economic variable selection},
  author={MacEachern, Steve N and Miyawaki, Koji},
  journal={arXiv preprint arXiv:1903.02136},
  year={2019}
}


@article{kowal2018bayesian,
  title={Bayesian Function-on-Scalars Regression for High Dimensional
                  Data},
  author={Kowal, Daniel R and Bourgeois, Daniel C},
  journal={arXiv preprint arXiv:1808.06689},
  year={2018}
}

@Manual{mgcv,
  title = {mgcv: Mixed GAM Computation Vehicle with Automatic
                  Smoothness Estimation},
  author = {Simon Wood},
  year = {2019},
  note = {R package version 1.8-28},
  url = {https://CRAN.R-project.org/package=mgcv},
}

@Manual{lars,
  title = {lars: Least Angle Regression, Lasso and Forward
                Stagewise},
  author = {Trevor Hastie and Brad Efron},
  year = {2013},
  note = {R package version 1.2},
  url = {https://CRAN.R-project.org/package=lars},
}


@article{lasso,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society: Series B
                  (Methodological)},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Wiley Online Library}
}


@article{chipman2010,
author = "Chipman, Hugh A. and George, Edward I. and McCulloch, Robert
                  E.",
doi = "10.1214/09-AOAS285",
fjournal = "The Annals of Applied Statistics",
journal = "Ann. Appl. Stat.",
month = "03",
number = "1",
pages = "266--298",
publisher = "The Institute of Mathematical Statistics",
title = "BART: Bayesian additive regression trees",
url = "https://doi.org/10.1214/09-AOAS285",
volume = "4",
year = "2010"
}

                  

@article{ICE,
author = {Alex Goldstein and Adam Kapelner and Justin Bleich and Emil
                  Pitkin},
title = {Peeking Inside the Black Box: Visualizing Statistical
                  Learning With Plots of Individual Conditional
                  Expectation},
journal = {Journal of Computational and Graphical Statistics},
volume = {24},
number = {1},
pages = {44-65},
year = {2015},
publisher = {Taylor & Francis},
doi = {10.1080/10618600.2014.907095},

URL = {
        https://doi.org/10.1080/10618600.2014.907095
    
},
eprint = {
        https://doi.org/10.1080/10618600.2014.907095
    
}

}

@book{molnar2019,
  title = {Interpretable Machine Learning},
  author = {Christoph Molnar},
  note = {\url{https://christophm.github.io/interpretable-ml-book/}},
  year = {2019},
  subtitle = {A Guide for Making Black Box Models Explainable},
  publisher={Self-published}
}

@ARTICLE{Chakraborty2016,
       author = {{Chakraborty}, Antik and {Bhattacharya}, Anirban and
                  {Mallick}, Bani K.},
        title = "{Bayesian sparse multiple regression for simultaneous
                  rank reduction and variable selection}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Methodology, Mathematics - Statistics
                  Theory},
         year = "2016",
        month = "Dec",
          eid = {arXiv:1612.00877},
        pages = {arXiv:1612.00877},
archivePrefix = {arXiv},
       eprint = {1612.00877},
 primaryClass = {stat.ME},
       adsurl =
                  {https://ui.adsabs.harvard.edu/abs/2016arXiv161200877C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{bashir2018post,
  title={Post-Processing Posteriors Over Precision Matrices to Produce
                  Sparse Graph Estimates},
  author={Bashir, Amir and Carvalho, Carlos M and Hahn, P Richard and
                  Jones, M Beatrix and others},
  journal={Bayesian Analysis},
  year={2018},
  publisher={International Society for Bayesian Analysis}
}


@article{maceachern2001decision,
  title={Decision theoretic aspects of dependent nonparametric
                  processes},
  author={MacEachern, Steven N},
  journal={Bayesian methods with applications to science, policy and
                  official statistics},
  pages={551--560},
  year={2001},
  publisher={Citeseer}
}


@article{MacEachern2016,
  title={Nonparametric Bayesian methods: a gentle introduction and
                  overview},
  author={MacEachern, Steven N},
  journal = {Communications for Statistical Applications and Methods},
  publisher={Korean Statistical Society},
  year={2016},
  pages={445-466},
  volume = {23},
  issue={6}
}


@article{GramacyLee2008,
author = {Robert B Gramacy and Herbert K. H Lee},
title = {Bayesian Treed Gaussian Process Models With an Application to
                  Computer Modeling},
journal = {Journal of the American Statistical Association},
volume = {103},
number = {483},
pages = {1119-1130},
year = {2008},
publisher = {Taylor & Francis},
doi = {10.1198/016214508000000689},

URL = {
        https://doi.org/10.1198/016214508000000689
    
},
eprint = {
        https://doi.org/10.1198/016214508000000689
    
}

}




@incollection{scottlee,
title = {A Unified Approach to Interpreting Model Predictions},
author = {Lundberg, Scott M and Lee, Su-In},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and
                  R. Fergus and S. Vishwanathan and R. Garnett},
pages = {4765--4774},
year = {2017},
publisher = {Curran Associates, Inc.},
url =
                  {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}
}


@ARTICLE{robustiml,
       author = {{Alvarez-Melis}, David and {Jaakkola}, Tommi S.},
        title = "{On the Robustness of Interpretability Methods}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics -
                  Machine Learning},
         year = "2018",
        month = "Jun",
          eid = {arXiv:1806.08049},
        pages = {arXiv:1806.08049},
archivePrefix = {arXiv},
       eprint = {1806.08049},
 primaryClass = {cs.LG},
       adsurl =
                  {https://ui.adsabs.harvard.edu/abs/2018arXiv180608049A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{ALE,
       author = {{Apley}, Daniel W.},
        title = "{Visualizing the Effects of Predictor Variables in
                  Black Box Supervised Learning Models}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Methodology},
         year = "2016",
        month = "Dec",
          eid = {arXiv:1612.08468},
        pages = {arXiv:1612.08468},
archivePrefix = {arXiv},
       eprint = {1612.08468},
 primaryClass = {stat.ME},
       adsurl =
                  {https://ui.adsabs.harvard.edu/abs/2016arXiv161208468A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


                  
@ARTICLE{lmposi,
       author = {{Panigrahi}, Snigdha and {Taylor}, Jonathan and
                  {Weinstein}, Asaf},
        title = "{Pliable Methods for Post-Selection Inference Under
                  Convex Constraints}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Methodology},
         year = "2016",
        month = "May",
          eid = {arXiv:1605.08824},
        pages = {arXiv:1605.08824},
archivePrefix = {arXiv},
       eprint = {1605.08824},
 primaryClass = {stat.ME},
       adsurl =
                  {https://ui.adsabs.harvard.edu/abs/2016arXiv160508824P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


                  
@ARTICLE{PuelzPortfolio,
       author = {{Puelz}, David and {Hahn}, P. Richard and {Carvalho},
                  Carlos M.},
        title = "{Portfolio Selection for Individual Passive
                  Investing}",
      journal = {SSRN},
         year = "2019",
        month = "April",
          eid = {2995484},
        pages = {2995484},
archivePrefix = {SSRN},
       eprint = {2995484},
       adsurl =
                  {https://ui.adsabs.harvard.edu/\#abs/2015arXiv151003385P},
}

                  
@ARTICLE{optimalETF,
       author = {{Puelz}, David and {Carvalho}, Carlos M. and {Hahn},
                  P. Richard},
        title = "{Optimal ETF Selection for Passive Investing}",
      journal = {arXiv e-prints},
     keywords = {Quantitative Finance - Statistical Finance,
                  Statistics - Applications},
         year = "2015",
        month = "Oct",
          eid = {arXiv:1510.03385},
        pages = {arXiv:1510.03385},
archivePrefix = {arXiv},
       eprint = {1510.03385},
 primaryClass = {q-fin.ST},
       adsurl =
                  {https://ui.adsabs.harvard.edu/\#abs/2015arXiv151003385P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{horseshoe,
    author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott,
                  James G.},
    title = "{The horseshoe estimator for sparse signals}",
    journal = {Biometrika},
    volume = {97},
    number = {2},
    pages = {465-480},
    year = {2010},
    month = {04},
    abstract = "{This paper proposes a new approach to sparsity,
                  called the horseshoe estimator, which arises from a
                  prior based on multivariate-normal scale
                  mixtures. We describe the estimator’s advantages
                  over existing approaches, including its robustness,
                  adaptivity to different sparsity patterns and
                  analytical tractability. We prove two theorems: one
                  that characterizes the horseshoe estimator’s tail
                  robustness and the other that demonstrates a
                  super-efficient rate of convergence to the correct
                  estimate of the sampling density in sparse
                  situations. Finally, using both real and simulated
                  data, we show that the horseshoe estimator
                  corresponds quite closely to the answers obtained by
                  Bayesian model averaging under a point-mass mixture
                  prior.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asq017},
    url = {https://doi.org/10.1093/biomet/asq017},
    eprint =
                  {http://oup.prod.sis.lan/biomet/article-pdf/97/2/465/584621/asq017.pdf},
}

@article{GramacyApley,
author = {Robert B. Gramacy and Daniel W. Apley},
title = {Local Gaussian Process Approximation for Large Computer
                  Experiments},
journal = {Journal of Computational and Graphical Statistics},
volume = 24,
number = 2,
pages = {561-578},
year = 2015,
publisher = {Taylor & Francis},
doi = {10.1080/10618600.2014.914442},
URL = {
        https://doi.org/10.1080/10618600.2014.914442
},
eprint = {
        https://doi.org/10.1080/10618600.2014.914442
    
}

}





@Article{Craven1978,
author="Craven, Peter
and Wahba, Grace",
title="Smoothing noisy data with spline functions",
journal="Numerische Mathematik",
year="1978",
month="Dec",
day="01",
volume="31",
number="4",
pages="377--403",
abstract="Smoothing splines are well known to provide nice curves
                  which smooth discrete, noisy data. We obtain a
                  practical, effective method for estimating the
                  optimum amount of smoothing from the
                  data. Derivatives can be estimated from the data by
                  differentiating the resulting (nearly) optimally
                  smoothed spline.",
issn="0945-3245",
doi="10.1007/BF01404567",
url="https://doi.org/10.1007/BF01404567"
}

@inproceedings{WalkerGutierrezPena,
author = {Walker, Stephen G. and Guti\'errez-Pe\~na, Eduardo},
title = {Robustifying {Bayesian} Procedures},
booktitle = {Bayesian Statistics 6},
year = {1999},
editor = {Bernardo, J.M. and Berger, J.O. and Dawid, A.P. and Smith,
                  A.F.M.},
pages = {685-710 (with discussion)},
publisher = {Oxford University Press}
}



@book{Wahba1990,
title = {Spline Models for Observational Data},
publish = {SIAM},
author = {Wahba, G.},
year = {1990},
doi = {10.1137/1.9781611970128.fm},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611970128.fm}
}


@article{Wood2001,
author = {Wood, Simon N.},
title = {Thin plate regression splines},
journal = {Journal of the Royal Statistical Society: Series B
                  (Statistical Methodology)},
volume = {65},
number = {1},
pages = {95-114},
keywords = {Generalized additive model, Regression spline, Thin plate
                  spline},
doi = {10.1111/1467-9868.00374},
url =
                  {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00374},
eprint =
                  {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00374},
abstract = {Summary. I discuss the production of low rank smoothers
                  for d ≥ 1 dimensional data, which can be fitted by
                  regression or penalized regression methods. The
                  smoothers are constructed by a simple transformation
                  and truncation of the basis that arises from the
                  solution of the thin plate spline smoothing problem
                  and are optimal in the sense that the truncation is
                  designed to result in the minimum possible
                  perturbation of the thin plate spline smoothing
                  problem given the dimension of the basis used to
                  construct the smoother. By making use of Lanczos
                  iteration the basis change and truncation are
                  computationally efficient. The smoothers allow the
                  use of approximate thin plate spline models with
                  large data sets, avoid the problems that are
                  associated with ‘knot placement’ that usually
                  complicate modelling with regression splines or
                  penalized regression splines, provide a sensible way
                  of modelling interaction terms in generalized
                  additive models, provide low rank approximations to
                  generalized smoothing spline models, appropriate for
                  use with large data sets, provide a means for
                  incorporating smooth functions of more than one
                  variable into non-linear models and improve the
                  computational efficiency of penalized likelihood
                  models incorporating thin plate splines. Given that
                  the approach produces spline-like models with a
                  sparse basis, it also provides a natural way of
                  incorporating unpenalized spline-like terms in
                  linear and generalized linear models, and these can
                  be treated just like any other model terms from the
                  point of view of model selection, inference and
                  diagnostics.},
year = {2003}
}

  @Book{Wood2017,
    title = {Generalized Additive Models: An Introduction with R},
    year = {2017},
    author = {S.N Wood},
    edition = {2},
    publisher = {Chapman and Hall/CRC},
  }

@article{DupuisRobert,
title = "Variable selection in qualitative models via an entropic
                  explanatory power",
journal = "Journal of Statistical Planning and Inference",
volume = "111",
number = "1",
pages = "77 - 94",
year = "2003",
note = "Special issue I: Model Selection, Model Diagnostics, Empirical
                  {Bayes} and Hierarchical {Bayes}",
issn = "0378-3758",
doi = "https://doi.org/10.1016/S0378-3758(02)00286-0",
url =
                  "http://www.sciencedirect.com/science/article/pii/S0378375802002860",
author = "J\'{e}rome A. Dupuis and Christian P. Robert",
keywords = "Additivity property, Entropy, Kullback–Leibler distance,
                  Logit model, Transitivity",
abstract = "The variable selection method proposed in the paper is
                  based on the evaluation of the Kullback–Leibler
                  distance between the full (or encompassing) model
                  and its submodels. The Bayesian implementation of
                  the method does not require a separate prior
                  modeling on the submodels since the corresponding
                  parameters for the submodels are defined as the
                  Kullback–Leibler projections of the full model
                  parameters. The result of the selection procedure is
                  the submodel with the smallest number of covariates
                  which is at an acceptable distance of the full
                  model. We introduce the notion of explanatory power
                  of a model and scale the maximal acceptable distance
                  in terms of the explanatory power of the full
                  model. Moreover, an additivity property between
                  embedded submodels shows that our selection
                  procedure is equivalent to select the submodel with
                  the smallest number of covariates which has a
                  sufficient explanatory power. We illustrate the
                  performances of this method on a breast cancer
                  dataset"
}

@article{GoutisRobert,
    author = {Goutis, Constantinos and Robert, Christian P.},
    title = "{Model choice in generalised linear models: A Bayesian
                  approach via Kullback-Leibler projections}",
    journal = {Biometrika},
    volume = 85,
    number = 1,
    pages = {29-37},
    year = 1998,
    month = 03,
    abstract = "{We propose a general Bayesian method of comparing
                  models. The approach is based on the
                  Kullback-Leibler distance between two families of
                  models, one nested within the other. For each
                  parameter value of a full model, we compute the
                  projection of the model to the restricted parameter
                  space and the corresponding minimum distance. From
                  the posterior distribution of the minimum distance,
                  we can judge whether or not a more parsimonious
                  model is appropriate. We show how the projection
                  method can be implemented for generalised linear
                  model selection and we propose some Markov chain
                  Monte Carlo algorithms for its practical
                  implementation in less tractable cases. We
                  illustrate the method with examples.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/85.1.29},
    url = {https://dx.doi.org/10.1093/biomet/85.1.29},
    eprint =
                  {http://oup.prod.sis.lan/biomet/article-pdf/85/1/29/656553/85-1-29.pdf},
}



@Article{Piironen2017,
author="Piironen, Juho
and Vehtari, Aki",
title="Comparison of {Bayesian} predictive methods for model
                  selection",
journal="Statistics and Computing",
year="2017",
month="May",
day="01",
volume="27",
number="3",
pages="711--735",
abstract="The goal of this paper is to compare several widely used
                  Bayesian model selection methods in practical model
                  selection problems, highlight their differences and
                  give recommendations about the preferred
                  approaches. We focus on the variable subset
                  selection for regression and classification and
                  perform several numerical experiments using both
                  simulated and real world data. The results show that
                  the optimization of a utility estimate such as the
                  cross-validation (CV) score is liable to finding
                  overfitted models due to relatively high variance in
                  the utility estimates when the data is scarce. This
                  can also lead to substantial selection induced bias
                  and optimism in the performance evaluation for the
                  selected model. From a predictive viewpoint, best
                  results are obtained by accounting for model
                  uncertainty by forming the full encompassing model,
                  such as the Bayesian model averaging solution over
                  the candidate models. If the encompassing model is
                  too complex, it can be robustly simplified by the
                  projection method, in which the information of the
                  full model is projected onto the submodels. This
                  approach is substantially less prone to overfitting
                  than selection based on CV-score. Overall, the
                  projection method appears to outperform also the
                  maximum a posteriori model and the selection of the
                  most probable variables. The study also demonstrates
                  that the model selection can greatly benefit from
                  using cross-validation outside the searching process
                  both for guiding the model size selection and
                  assessing the predictive performance of the finally
                  selected model.",
issn="1573-1375",
doi="10.1007/s11222-016-9649-y",
url="https://doi.org/10.1007/s11222-016-9649-y"
}


@article{Vehtari2012,
author = "Vehtari, Aki and Ojanen, Janne",
doi = "10.1214/12-SS102",
fjournal = "Statistics Surveys",
journal = "Statist. Surv.",
pages = "142--228",
publisher = "The American Statistical Association, the Bernoulli
                  Society, the Institute of Mathematical Statistics,
                  and the Statistical Society of Canada",
title = "A survey of {Bayesian} predictive methods for model
                  assessment, selection and comparison",
url = "https://doi.org/10.1214/12-SS102",
volume = "6",
year = "2012"
}



@article{ClydeGeorge,
 ISSN = {08834237},
 URL = {http://www.jstor.org/stable/4144374},
 abstract = {The evolution of Bayesian approaches for model
                  uncertainty over the past decade has been
                  remarkable. Catalyzed by advances in methods and
                  technology for posterior computation, the scope of
                  these methods has widened substantially. Major
                  thrusts of these developments have included new
                  methods for semiautomatic prior specification and
                  posterior exploration. To illustrate key aspects of
                  this evolution, the highlights of some of these
                  developments are described.},
 author = {Merlise Clyde and Edward I. George},
 journal = {Statistical Science},
 number = {1},
 pages = {81--94},
 publisher = {Institute of Mathematical Statistics},
 title = {Model Uncertainty},
 volume = {19},
 year = {2004}
}


@article{NottLeng,
title = "Bayesian projection approaches to variable selection in
                  generalized linear models",
journal = "Computational Statistics & Data Analysis",
volume = "54",
number = "12",
pages = "3227 - 3241",
year = "2010",
issn = "0167-9473",
doi = "https://doi.org/10.1016/j.csda.2010.01.036",
url =
                  "http://www.sciencedirect.com/science/article/pii/S016794731000054X",
author = "David J. Nott and Chenlei Leng",
keywords = "Bayesian variable selection, Kullback–Leibler projection,
                  Lasso, Non-negative garotte, Preconditioning",
abstract = "A Bayesian approach to variable selection which is based
                  on the expected Kullback–Leibler divergence between
                  the full model and its projection onto a submodel
                  has recently been suggested in the literature. For
                  generalized linear models an extension of this idea
                  is proposed by considering projections onto
                  subspaces defined via some form of L1 constraint on
                  the parameter in the full model. This leads to
                  Bayesian model selection approaches related to the
                  lasso. In the posterior distribution of the
                  projection there is positive probability that some
                  components are exactly zero and the posterior
                  distribution on the model space induced by the
                  projection allows exploration of model
                  uncertainty. Use of the approach in structured
                  variable selection problems such as ANOVA models is
                  also considered, where it is desired to incorporate
                  main effects in the presence of
                  interactions. Projections related to the
                  non-negative garotte are able to respect the
                  hierarchical constraints. A consistency result is
                  given concerning the posterior distribution on the
                  model induced by the projection, showing that for
                  some projections related to the adaptive lasso and
                  non-negative garotte the posterior distribution
                  concentrates on the true model asymptotically."
}

@book{BayesianChoice,
author = {Robert, Christian},
year = {2001},
month = {01},
pages = {},
title = {The Bayesian Choice},
publisher = {Springer},
doi = {10.1007/978-1-4757-4314-2}
}

@article{PuelzSUR,
author = "Puelz, David and Hahn, P. Richard and Carvalho, Carlos M.",
doi = "10.1214/17-BA1053",
fjournal = "Bayesian Analysis",
journal = "Bayesian Anal.",
month = "12",
number = "4",
pages = "969--989",
publisher = "International Society for Bayesian Analysis",
title = "Variable Selection in Seemingly Unrelated Regressions with
                  Random Predictors",
url = "https://doi.org/10.1214/17-BA1053",
volume = "12",
year = "2017"
}



@ARTICLE{Piironenetal2018,
       author = {{Piironen}, Juho and {Paasiniemi}, Markus and
                  {Vehtari}, Aki},
        title = "{Projective Inference in High-dimensional Problems:
                  Prediction and Feature Selection}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science -
                  Machine Learning},
         year = 2018,
        month = Oct,
          eid = {arXiv:1810.02406},
        pages = {arXiv:1810.02406},
archivePrefix = {arXiv},
       eprint = {1810.02406},
 primaryClass = {stat.ML},
       adsurl =
                  {https://ui.adsabs.harvard.edu/\#abs/2018arXiv181002406P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{Friedman2001,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2699986},
 abstract = {Function estimation/approximation is viewed from the
                  perspective of numerical optimization in function
                  space, rather than parameter space. A connection is
                  made between stagewise additive expansions and
                  steepest-descent minimization. A general gradient
                  descent "boosting" paradigm is developed for
                  additive expansions based on any fitting
                  criterion. Specific algorithms are presented for
                  least-squares, least absolute deviation, and Huber-M
                  loss functions for regression, and multiclass
                  logistic likelihood for classification. Special
                  enhancements are derived for the particular case
                  where the individual additive components are
                  regression trees, and tools for interpreting such
                  "TreeBoost" models are presented. Gradient boosting
                  of regression trees produces competitive, highly
                  robust, interpretable procedures for both regression
                  and classification, especially appropriate for
                  mining less than clean data. Connections between
                  this approach and the boosting methods of Freund and
                  Shapire and Friedman, Hastie and Tibshirani are
                  discussed.},
 author = {Jerome H. Friedman},
 journal = {The Annals of Statistics},
 number = {5},
 pages = {1189--1232},
 publisher = {Institute of Mathematical Statistics},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 volume = {29},
 year = {2001}
}

                  
@article{RIClinear,
author = "Hahn, P. Richard and Carvalho, Carlos M. and Puelz, David
                  and He, Jingyu",
doi = "10.1214/16-BA1044",
fjournal = "Bayesian Analysis",
journal = "Bayesian Anal.",
month = "03",
number = "1",
pages = "163--182",
publisher = "International Society for Bayesian Analysis",
title = "Regularization and Confounding in Linear Regression for
                  Treatment Effect Estimation",
url = "https://doi.org/10.1214/16-BA1044",
volume = "13",
year = "2018"
}



@article{lee2016,
author = "Lee, Jason D. and Sun, Dennis L. and Sun, Yuekai and Taylor,
                  Jonathan E.",
doi = "10.1214/15-AOS1371",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "06",
number = "3",
pages = "907--927",
publisher = "The Institute of Mathematical Statistics",
title = "Exact post-selection inference, with application to the
                  lasso",
url = "https://doi.org/10.1214/15-AOS1371",
volume = "44",
year = "2016"
}


@article{adaptivelasso,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/27639762},
 abstract = {The lasso is a popular technique for simultaneous
                  estimation and variable selection. Lasso variable
                  selection has been shown to be consistent under
                  certain conditions. In this work we derive a
                  necessary condition for the lasso variable selection
                  to be consistent. Consequently, there exist certain
                  scenarios where the lasso is inconsistent for
                  variable selection. We then propose a new version of
                  the lasso, called the adaptive lasso, where adaptive
                  weights are used for penalizing different
                  coefficients in the l1 penalty. We show that the
                  adaptive lasso enjoys the oracle properties; namely,
                  it performs as well as if the true underlying model
                  were given in advance. Similar to the lasso, the
                  adaptive lasso is shown to be near-minimax
                  optimal. Furthermore, the adaptive lasso can be
                  solved by the same efficient algorithm for solving
                  the lasso. We also discuss the extension of the
                  adaptive lasso in generalized linear models and show
                  that the oracle properties still hold under mild
                  regularity conditions. As a byproduct of our theory,
                  the nonnegative garotte is shown to be consistent
                  for variable selection.},
 author = {Hui Zou},
 journal = {Journal of the American Statistical Association},
 number = {476},
 pages = {1418--1429},
 publisher = {[American Statistical Association, Taylor & Francis,
                  Ltd.]},
 title = {The Adaptive Lasso and Its Oracle Properties},
 volume = {101},
 year = {2006}
}

@article{einstein,
    author = "Albert Einstein",
    title = "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
    [{On} the electrodynamics of moving bodies]",
    journal = "Annalen der Physik",
    volume = "322",
    number = "10",
    pages = "891--921",
    year = "1905",
    DOI = "http://dx.doi.org/10.1002/andp.19053221004",
    keywords = "physics"
}

@article{DSS,
author = {P. Richard Hahn and Carlos M. Carvalho},
title = {Decoupling Shrinkage and Selection in {Bayesian} Linear
                  Models: A Posterior Summary Perspective},
journal = {Journal of the American Statistical Association},
volume = {110},
number = {509},
pages = {435-448},
year = {2015},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2014.993077},
URL = {https://doi.org/10.1080/01621459.2014.993077},
eprint = {https://doi.org/10.1080/01621459.2014.993077}
}

@inproceedings{lime,
 author = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
 title = { "{Why} Should {I} Trust You?": Explaining the Predictions
                  of Any Classifier },
 booktitle = {Knowledge Discovery and Data Mining (KDD)},
 year = {2016}
}



@incollection{Galyardt14mmm,
	Author = {April Galyardt},
	Booktitle = {Handbook of Mixed Membership Models},
	Date-Added = {2014-08-21 21:18:27 +0000},
	Date-Modified = {2014-08-21 21:18:27 +0000},
	Editor = {Edoardo M. Airoldi and David Blei and Erosheva,
                  Elena and Fienberg, Stephen E.},
	Publisher = {Chapman and Hall},
	Title = {Interpreting Mixed Membership Models: Implications of
                  Erosheva's Representation Theorem},
	Year = {2014},
	Bdsk-File-1 =
                  {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QMi4uLy4uL1BhcGVyLU1NIGNoYXB0ZXIvR2FseWFyZHQtY2hhcHRlci03LjguMTMucGRm0hcLGBlXTlMuZGF0YU8RAeIAAAAAAeIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMm3i1VIKwAAAJ3/thtHYWx5YXJkdC1jaGFwdGVyLTcuOC4xMy5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAu7Y5zgBejQAAAAAAAAAAAAIAAgAACSAAAAAAAAAAAAAAAAAAAAAQUGFwZXItTU0gY2hhcHRlcgAQAAgAAMm3w5UAAAARAAgAAM4Als0AAAABABAAnf+2ABefWwANUdgAAJg6AAIAVU1hY2ludG9zaCBIRDpVc2VyczoAYWdhbHlhcmR0OgBEcm9wYm94OgBQYXBlci1NTSBjaGFwdGVyOgBHYWx5YXJkdC1jaGFwdGVyLTcuOC4xMy5wZGYAAA4AOAAbAEcAYQBsAHkAYQByAGQAdAAtAGMAaABhAHAAdABlAHIALQA3AC4AOAAuADEAMwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIARFVzZXJzL2FnYWx5YXJkdC9Ecm9wYm94L1BhcGVyLU1NIGNoYXB0ZXIvR2FseWFyZHQtY2hhcHRlci03LjguMTMucGRmABMAAS8AABUAAgAQ//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4AwwDIANACtgK4Ar0CyALRAt8C4wLqAvMC+AMFAwgDGgMdAyIAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADJA==}}

@phdthesis{Galyardt12dis,
	Address = {Pittsburgh, PA 15213},
	Author = {April Galyardt},
	Date-Added = {2014-08-21 21:17:56 +0000},
	Date-Modified = {2014-08-22 02:49:38 +0000},
	Keywords = {mixed membership, Latent Dirichlet Allocation,
                  cognitive diagnosis models, numerical estimation,
                  psychometrics},
	Month = {July},
	School = {Carnegie Mellon University},
	Title = {Mixed Membership Distributions with Applications to
                  Modeling Multiple Strategy Usage},
	Year = {2012},
	Bdsk-File-1 =
                  {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QRC4uLy4uLy4uL0RvY3VtZW50cy9QdWJsaWNhdGlvbnMvMjAxMi1HYWx5YXJkdC1EaXNzZXJ0YXRpb24tRmluYWwucGRm0hcLGBlXTlMuZGF0YU8RAfoAAAAAAfoAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMtppx9IKwAAAAuN+h8yMDEyLUdhbHlhcmR0LURpc3NlciMyNjgyNTgucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJoJYzMbb7wAAAAAAAAAAAAMAAwAACSAAAAAAAAAAAAAAAAAAAAAMUHVibGljYXRpb25zABAACAAAy2ntbwAAABEACAAAzMciPwAAAAEAEAALjfoABcifAAXIngAAuVcAAgBXTWFjaW50b3NoIEhEOlVzZXJzOgBhZ2FseWFyZHQ6AERvY3VtZW50czoAUHVibGljYXRpb25zOgAyMDEyLUdhbHlhcmR0LURpc3NlciMyNjgyNTgucGRmAAAOAEoAJAAyADAAMQAyAC0ARwBhAGwAeQBhAHIAZAB0AC0ARABpAHMAcwBlAHIAdABhAHQAaQBvAG4ALQBGAGkAbgBhAGwALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEtVc2Vycy9hZ2FseWFyZHQvRG9jdW1lbnRzL1B1YmxpY2F0aW9ucy8yMDEyLUdhbHlhcmR0LURpc3NlcnRhdGlvbi1GaW5hbC5wZGYAABMAAS8AABUAAgAQ//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4A1QDaAOIC4ALiAucC8gL7AwkDDQMUAx0DIgMvAzIDRANHA0wAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADTg==}}

@conference{Galyardt13smmc,
	Address = {Arnhem, Netherlands},
	Author = {April Galyardt and Ilya Goldin},
	Booktitle = {International Meeting of the Psychometric
                  Society},
	Date-Added = {2014-08-21 21:17:50 +0000},
	Date-Modified = {2014-08-21 21:17:50 +0000},
	Keywords = {conf},
	Title = {Modeling Student Metacognitive Strategies in a
                  Intelligent Tutoring System},
	Year = {2013}}

@article{Campbell02,
	Author = {Jamie I.D. Campbell and Shauna Austin},
	Date-Added = {2014-08-21 21:12:16 +0000},
	Date-Modified = {2014-08-21 21:13:22 +0000},
	Journal = {Memory \& Cognition},
	Keywords = {strategies, addition, response time},
	Number = {6},
	Pages = {988-994},
	Title = {Effects of response time deadlines on adults'
                  strategy choices for simple addition},
	Volume = {30},
	Year = {2002}}

@article{Schubert13,
	Author = {Schubert, Christiane C and Denmark, T Kent and
                  Crandall, Beth and Grome, Anna and Pappas, James},
	Date-Added = {2014-08-21 21:00:46 +0000},
	Date-Modified = {2014-08-21 21:00:46 +0000},
	Journal = {Annals of emergency medicine},
	Keywords = {expert/novice differences},
	Number = {1},
	Pages = {96--109},
	Publisher = {Elsevier},
	Title = {Characterizing novice-expert differences in
                  macrocognition: an exploratory study of cognitive
                  work in the emergency department},
	Volume = {61},
	Year = {2013},
	Bdsk-File-1 =
                  {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QYS4uLy4uL1BhcGVyLVdoYXQncyBhIFN0cmF0ZWd5L1BhcGVycy1TdHJhdGVnaWVzL1JlYWQvU2NodWJlcnQgZXRhbCAoMjAxMikgLUVtZXJnZW5jeSBNZWRpY2luZS5wZGbSFwsYGVdOUy5kYXRhTxECVAAAAAACVAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAybeLVUgrAAACeKY6H1NjaHViZXJ0IGV0YWwgKDIwMTIjMjc4QTdCNi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJ4p7bP6W1eAAAAAAAAAAAAAgAEAAAJIAAAAAAAAAAAAAAAAAAAAARSZWFkABAACAAAybfDlQAAABEACAAAz+mlngAAAAEAGAJ4pjoCeKY4ANZobAAXn1sADVHYAACYOgACAHlNYWNpbnRvc2ggSEQ6VXNlcnM6AGFnYWx5YXJkdDoARHJvcGJveDoAUGFwZXItV2hhdCdzIGEgU3RyYXRlZ3k6AFBhcGVycy1TdHJhdGVnaWVzOgBSZWFkOgBTY2h1YmVydCBldGFsICgyMDEyIzI3OEE3QjYucGRmAAAOAFoALABTAGMAaAB1AGIAZQByAHQAIABlAHQAYQBsACAAKAAyADAAMQAyACkAIAAtAEUAbQBlAHIAZwBlAG4AYwB5ACAATQBlAGQAaQBjAGkAbgBlAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBzVXNlcnMvYWdhbHlhcmR0L0Ryb3Bib3gvUGFwZXItV2hhdCdzIGEgU3RyYXRlZ3kvUGFwZXJzLVN0cmF0ZWdpZXMvUmVhZC9TY2h1YmVydCBldGFsICgyMDEyKSAtRW1lcmdlbmN5IE1lZGljaW5lLnBkZgAAEwABLwAAFQACABD//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgDyAPcA/wNXA1kDXgNpA3IDgAOEA4sDlAOZA6YDqQO7A74DwwAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAPF}}

@article{Siegler87,
	Author = {Robert S. Siegler},
	Date-Added = {2014-08-21 21:00:35 +0000},
	Date-Modified = {2014-08-21 21:00:35 +0000},
	Journal = {Journal of Experimental Psychology: General},
	Keywords = {Multiple strategy use; mathematics education;
                  strategies; learning sciences; psychology},
	Number = {3},
	Pages = {250-264},
	Title = {The perils of averaging data over strategies: An
                  example from children's addition},
	Volume = {116},
	Year = {1987},
	Bdsk-File-1 =
                  {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8Qby4uLy4uLy4uL0RvY3VtZW50cy9Kb3VybmFsLU5ld3MtQXJ0aWNsZXMvTGVhcm5pbmcgU2NpZW5jZXMsIExBLCBFRE0vU2llZ2xlci04Ny1NdWx0aXBsZSBBZGRpdGlvbiBTdHJhdGVnaWVzLnBkZtIXCxgZV05TLmRhdGFPEQJoAAAAAAJoAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADLaacfSCsAAAALnbIfU2llZ2xlci04Ny1NdWx0aXBsZSBBI0I5RTE1LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAueFcjZ9odQREYgAAAAAAADAAQAAAkgAAAAAAAAAAAAAAAAAAAAGkxlYXJuaW5nIFNjaWVuY2VzLCBMQSwgRURNABAACAAAy2ntbwAAABEACAAAyNouxwAAAAEAFAALnbIAC5wCAAXInwAFyJ4AALlXAAIAfE1hY2ludG9zaCBIRDpVc2VyczoAYWdhbHlhcmR0OgBEb2N1bWVudHM6AEpvdXJuYWwtTmV3cy1BcnRpY2xlczoATGVhcm5pbmcgU2NpZW5jZXMsIExBLCBFRE06AFNpZWdsZXItODctTXVsdGlwbGUgQSNCOUUxNS5wZGYADgBYACsAUwBpAGUAZwBsAGUAcgAtADgANwAtAE0AdQBsAHQAaQBwAGwAZQAgAEEAZABkAGkAdABpAG8AbgAgAFMAdAByAGEAdABlAGcAaQBlAHMALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAHZVc2Vycy9hZ2FseWFyZHQvRG9jdW1lbnRzL0pvdXJuYWwtTmV3cy1BcnRpY2xlcy9MZWFybmluZyBTY2llbmNlcywgTEEsIEVETS9TaWVnbGVyLTg3LU11bHRpcGxlIEFkZGl0aW9uIFN0cmF0ZWdpZXMucGRmABMAAS8AABUAAgAQ//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4BAAEFAQ0DeQN7A4ADiwOUA6IDpgOtA7YDuwPIA8sD3QPgA+UAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAD5w==}}

@article{Chi81,
	Author = {Chi, Michelene T.H. and Feltovich, Paul J. and
                  Glaser, Robert},
	Date-Added = {2014-08-21 21:00:27 +0000},
	Date-Modified = {2014-08-21 21:00:27 +0000},
	Journal = {Cognitive science},
	Keywords = {expert/novice differences},
	Number = {2},
	Pages = {121--152},
	Publisher = {Wiley Online Library},
	Title = {Categorization and representation of physics problems
                  by experts and novices},
	Volume = {5},
	Year = {1981},
	Bdsk-File-1 =
                  {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QZy4uLy4uL1BhcGVyLVdoYXQncyBhIFN0cmF0ZWd5L1BhcGVycy1TdHJhdGVnaWVzL1JlYWQvQ2hpLCBGZWx0b3ZpY2gsIEdsYXNlciAoMTk4MSktcGh5c2ljcyBwcm9ibGVtcy5wZGbSFwsYGVdOUy5kYXRhTxECZgAAAAACZgACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAybeLVUgrAAACeKY6H0NoaSwgRmVsdG92aWNoLCBHbGEjMjc4QTdDQy5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJ4p8zP6ZB8AAAAAAAAAAAAAgAEAAAJIAAAAAAAAAAAAAAAAAAAAARSZWFkABAACAAAybfDlQAAABEACAAAz+nIvAAAAAEAGAJ4pjoCeKY4ANZobAAXn1sADVHYAACYOgACAHlNYWNpbnRvc2ggSEQ6VXNlcnM6AGFnYWx5YXJkdDoARHJvcGJveDoAUGFwZXItV2hhdCdzIGEgU3RyYXRlZ3k6AFBhcGVycy1TdHJhdGVnaWVzOgBSZWFkOgBDaGksIEZlbHRvdmljaCwgR2xhIzI3OEE3Q0MucGRmAAAOAGYAMgBDAGgAaQAsACAARgBlAGwAdABvAHYAaQBjAGgALAAgAEcAbABhAHMAZQByACAAKAAxADkAOAAxACkALQBwAGgAeQBzAGkAYwBzACAAcAByAG8AYgBsAGUAbQBzAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgB5VXNlcnMvYWdhbHlhcmR0L0Ryb3Bib3gvUGFwZXItV2hhdCdzIGEgU3RyYXRlZ3kvUGFwZXJzLVN0cmF0ZWdpZXMvUmVhZC9DaGksIEZlbHRvdmljaCwgR2xhc2VyICgxOTgxKS1waHlzaWNzIHByb2JsZW1zLnBkZgAAEwABLwAAFQACABD//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgD4AP0BBQNvA3EDdgOBA4oDmAOcA6MDrAOxA74DwQPTA9YD2wAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAPd}}

@incollection{Mislevy06Cog,
	Author = {Robert Mislevy},
	Booktitle = {Educational Assessment},
	Chapter = {8},
	Date-Added = {2014-08-21 20:58:59 +0000},
	Date-Modified = {2014-08-21 20:58:59 +0000},
	Editor = {Robert L. Brennan},
	Publisher = {American Council on Education and Praeger
                  Publishers},
	Title = {Cognitive Psychology and Educational Assessment},
	Year = {2006},
	Bdsk-File-1 =
                  {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QUy4uLy4uL1BhcGVyLVdoYXQncyBhIFN0cmF0ZWd5L1BhcGVycy1Nb2RlbHMvTWlzbGV2eSAoMjAwNiktQ29nIFBzeSAmIEFzc2Vzc21lbnQucGRm0hcLGBlXTlMuZGF0YU8RAjgAAAAAAjgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMm3i1VIKwAAAnimNh9NaXNsZXZ5ICgyMDA2KS1Db2cgIzI3QjBENEIucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACew1Lz+7ulQAAAAAAAAAAAAIAAwAACSAAAAAAAAAAAAAAAAAAAAANUGFwZXJzLU1vZGVscwAAEAAIAADJt8OVAAAAEQAIAADP7ybVAAAAAQAUAnimNgDWaGwAF59bAA1R2AAAmDoAAgBvTWFjaW50b3NoIEhEOlVzZXJzOgBhZ2FseWFyZHQ6AERyb3Bib3g6AFBhcGVyLVdoYXQncyBhIFN0cmF0ZWd5OgBQYXBlcnMtTW9kZWxzOgBNaXNsZXZ5ICgyMDA2KS1Db2cgIzI3QjBENEIucGRmAAAOAFAAJwBNAGkAcwBsAGUAdgB5ACAAKAAyADAAMAA2ACkALQBDAG8AZwAgAFAAcwB5ACAAJgAgAEEAcwBzAGUAcwBzAG0AZQBuAHQALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAGVVc2Vycy9hZ2FseWFyZHQvRHJvcGJveC9QYXBlci1XaGF0J3MgYSBTdHJhdGVneS9QYXBlcnMtTW9kZWxzL01pc2xldnkgKDIwMDYpLUNvZyBQc3kgJiBBc3Nlc3NtZW50LnBkZgAAEwABLwAAFQACABD//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgDkAOkA8QMtAy8DNAM/A0gDVgNaA2EDagNvA3wDfwORA5QDmQAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAOb}}

@article{Mislevy12,
	Author = {Robert J. Mislevy and John T. Behrens and Kristen
                  E. {DiCerbo} and Roy Levy},
	Date-Added = {2014-03-27 15:22:33 +0000},
	Date-Modified = {2014-03-27 15:22:33 +0000},
	Journal = {Journal of Educational Data Mining},
	Number = {1},
	Title = {Design and Discovery in Educational Assessment:
                  Evidence-Centered Design, Psychometrics, and
                  Educational Data Mining},
	Volume = {4},
	Year = {2012},
	Bdsk-File-1 =
                  {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QWC4uLy4uL1ByaXZhdGUtRVJTSDg5OTAvUmVhZGluZ0xpc3QvQ29waWVzX1VubWFya2VkX0Rpc3RyaWJ1dGUvUjEtTWlzbGV2eSBldGFsICgyMDEyKS5wZGbSFwsYGVdOUy5kYXRhTxECQAAAAAACQAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAybeLVUgrAAACIbwqGlIxLU1pc2xldnkgZXRhbCAoMjAxMikucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIhvEDPPjaUAAAAAAAAAAAAAgAEAAAJIAAAAAAAAAAAAAAAAAAAABpDb3BpZXNfVW5tYXJrZWRfRGlzdHJpYnV0ZQAQAAgAAMm3w5UAAAARAAgAAM8+fOQAAAABABgCIbwqAiG8JQCmuWsAF59bAA1R2AAAmDoAAgB9TWFjaW50b3NoIEhEOlVzZXJzOgBhZ2FseWFyZHQ6AERyb3Bib3g6AFByaXZhdGUtRVJTSDg5OTA6AFJlYWRpbmdMaXN0OgBDb3BpZXNfVW5tYXJrZWRfRGlzdHJpYnV0ZToAUjEtTWlzbGV2eSBldGFsICgyMDEyKS5wZGYAAA4ANgAaAFIAMQAtAE0AaQBzAGwAZQB2AHkAIABlAHQAYQBsACAAKAAyADAAMQAyACkALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAGpVc2Vycy9hZ2FseWFyZHQvRHJvcGJveC9Qcml2YXRlLUVSU0g4OTkwL1JlYWRpbmdMaXN0L0NvcGllc19Vbm1hcmtlZF9EaXN0cmlidXRlL1IxLU1pc2xldnkgZXRhbCAoMjAxMikucGRmABMAAS8AABUAAgAQ//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4A6QDuAPYDOgM8A0EDTANVA2MDZwNuA3cDfAOJA4wDngOhA6YAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADqA==}}

@incollection{VanLehn08,
	Author = {Kurt VanLehn},
	Booktitle = {The future of assessment: Shaping teaching and
                  learning.},
	Date-Added = {2014-03-27 14:52:29 +0000},
	Date-Modified = {2014-03-27 14:54:32 +0000},
	Editor = {C. Dwyer},
	Keywords = {intelligent tutoring systems},
	Pages = {113-138},
	Publisher = {Erbaum},
	Title = {Intelligent Tutoring Systems for Continuous, Embedded
                  Assessment},
	Year = {2008},
	Bdsk-File-1 =
                  {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QXS4uLy4uL05ldyBBcnRpY2xlcy9SZWFkL1ZhbkxlaG4oMjAwOCktIEludGVsbGlnZW50IHR1dG9yaW5nIHN5c3RlbXMgZm9yIGNvbnRpbnVvdXMsIGVtYmVkLnBkZtIXCxgZV05TLmRhdGFPEQJgAAAAAAJgAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADJt4tVSCsAAAE5yhcfVmFuTGVobigyMDA4KS0gSW50ZSMyMkY0MUFFLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAi9Brs9Zst0AAAAAAAAAAAACAAMAAAkgAAAAAAAAAAAAAAAAAAAABFJlYWQAEAAIAADJt8OVAAAAEQAIAADPWesdAAAAAQAUATnKFwAb8HgAF59bAA1R2AAAmDoAAgBbTWFjaW50b3NoIEhEOlVzZXJzOgBhZ2FseWFyZHQ6AERyb3Bib3g6AE5ldyBBcnRpY2xlczoAUmVhZDoAVmFuTGVobigyMDA4KS0gSW50ZSMyMkY0MUFFLnBkZgAADgCMAEUAVgBhAG4ATABlAGgAbgAoADIAMAAwADgAKQAtACAASQBuAHQAZQBsAGwAaQBnAGUAbgB0ACAAdAB1AHQAbwByAGkAbgBnACAAcwB5AHMAdABlAG0AcwAgAGYAbwByACAAYwBvAG4AdABpAG4AdQBvAHUAcwAsACAAZQBtAGIAZQBkAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBvVXNlcnMvYWdhbHlhcmR0L0Ryb3Bib3gvTmV3IEFydGljbGVzL1JlYWQvVmFuTGVobigyMDA4KS0gSW50ZWxsaWdlbnQgdHV0b3Jpbmcgc3lzdGVtcyBmb3IgY29udGludW91cywgZW1iZWQucGRmAAATAAEvAAAVAAIAEP//AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAO4A8wD7A18DYQNmA3EDegOIA4wDkwOcA6EDrgOxA8MDxgPLAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAA80=}}

@article{Erosheva04,
	Author = {Elena Erosheva and Stephen E. Fienberg and John
                  Lafferty},
	Date-Added = {2013-12-18 14:22:42 +0000},
	Date-Modified = {2013-12-18 14:25:57 +0000},
	Journal = {PNAS},
	Number = {Suppl.1},
	Pages = {5220-5227},
	Title = {Mixed-Membership Models of Scientific Publications},
	Volume = {101},
	Year = {2004},
	Bdsk-Url-1 =
                  {http://www.pnas.org/cgi/content/full/101/suppl_1/5220}}

@incollection{Blei09,
	Author = {David Blei and John Lafferty},
	Booktitle = {Text Mining: Classification, Clustering, and
                  Applications},
	Date-Added = {2013-12-18 14:09:12 +0000},
	Date-Modified = {2013-12-18 14:12:00 +0000},
	Editor = {A. Srivastava and M. Sahami},
	Publisher = {Chapman \& Hall/CRC},
	Series = {Data Mining and Knowledge Discovery Series},
	Title = {Topic Models},
	Year = {2009},
	Bdsk-Url-1 =
                  {http://www.cs.princeton.edu/~blei/papers/BleiLafferty2009.pdf}}

@article{aleven_toward_2006,
	Author = {Aleven, V. and Mclaren, B. and Roll, I. and
                  Koedinger, K.},
	Date-Added = {2013-12-05 22:14:42 +0000},
	Date-Modified = {2013-12-17 17:12:03 +0000},
	Issue = {2},
	Journal = {International Journal of Artificial Intelligence in
                  Education},
	Pages = {101-128},
	Title = {Toward meta-cognitive tutoring: A model of help
                  seeking with a Cognitive Tutor},
	Volume = {16},
	Year = {2006}}

@inproceedings{goldin_hints:_2013,
	Author = {Goldin, Ilya M. and Koedinger, Kenneth R. and
                  Aleven, Vincent A. W. M. M.},
	Booktitle = {Proceedings of 6th International Conference on
                  Educational Data Mining},
	Date-Added = {2013-12-05 22:14:42 +0000},
	Date-Modified = {2013-12-17 19:51:31 +0000},
	Editor = {{D'Mello}, Sidney K. and Calvo, Rafael A. and Olney,
                  Andrew},
	Location = {Memphis, {TN}},
	Month = {July},
	Title = {Hints: You Can't Have Just One},
	Year = {2013},
	Bdsk-Url-1 =
                  {http://www.educationaldatamining.org/EDM2013/papers/rn_paper_35.pdf}}

@inproceedings{goldin_learner_2012,
	Address = {Chania, Greece},
	Author = {Goldin, Ilya M. and Koedinger, Kenneth R. and
                  Aleven, Vincent A. W. M. M.},
	Booktitle = {Proceedings of 5th International Conference on
                  Educational Data Mining},
	Date-Added = {2013-12-05 22:14:42 +0000},
	Date-Modified = {2013-12-17 19:50:21 +0000},
	Editor = {Yacef, Kalina and Za{\"\i}ane, Osmar and
                  Hershkovitz, Arnon and Yudelson, Michael and
                  Stamper, John},
	Pages = {73-80},
	Title = {Learner Differences in Hint Processing},
	Year = {2012},
	Bdsk-Url-1 =
                  {http://educationaldatamining.org/EDM2012/uploads/procs/Full_Papers/edm2012_full_6.pdf}}

@inproceedings{goldin_learner_2013,
	Address = {Memphis, {TN}},
	Author = {Goldin, Ilya M. and Carlson, Ryan},
	Booktitle = {Proceedings of 16th International Conference on
                  Artificial Intelligence in Education},
	Date-Added = {2013-12-05 22:14:42 +0000},
	Date-Modified = {2013-12-17 17:14:16 +0000},
	Title = {Learner Differences and Hint Content},
	Year = {2013}}

@article{Girolami05,
	Abstract = {To provide a parsimonious generative
                  representation of the sequential activity of a
                  number of individuals within a population there is a
                  necessary tradeoff between the definition of
                  individual specific and global
representations. A linear-time algorithm is proposed that defines a
                  distributed predictive model for finite state
                  symbolic sequences which represent the traces of the
                  activity of a number of individuals within a
                  group. The algorithm is based on a straightforward
                  generalization of latent Dirichlet allocation to
                  time-invariant Markov chains of arbitrary order. The
                  modelling assumption made is that the possibly
                  heterogeneous behavior of individuals may be
                  represented by a relatively small number of simple
                  and common behavioral traits which may interleave
                  randomly according to an individual-specific
                  distribution. The results of an empirical study on
                  three different application domains indicate that
                  this modelling approach provides an efficient
                  low-complexity and intuitively interpretable
                  representation scheme which is reflected by improved
                  prediction performance over comparable models.},
	Annote = {Builds a mixed-membership model where the basis
                  profiles are markov chains of varying order.
Uses two types of estimation: Variational approximation, and "maximum
                  a posteriori" (MAP)
Fits model on 3 different data sets of user-log data: actions in a
                  word processor, sequences of calls to different
                  geographic regions (UK), and finally web page
                  browsing.

Compares models on "perplexity" which they define as the exponential
                  of the negative-normalized log-likelihood.  (Might
                  be a standard markov-chain goodness of fit measure,
                  or it might be more standard to ML)
They use t-tests and non-parametric Wilcoxon Rank-Sum tests to test
                  for significant differences in perplexity.

They call their model a "simplical mixture of markov chains" and cite
                  Minka and Lafferty 2002.

For the zero^th order Markov model (when no memory is assumed in the
                  markov process), then their model reduces to a
                  multinomial LDA model.

They also claim that Hofmann's pLSA algorithm is equivalent to LDA
                  when the MAP estimator is calculated through an
                  iterative convergence method. Suggests that LDA is
                  an improvement over pLSA because of the estimation
                  method used.  Cites Lappalainen and Miskin (2000) on
                  the weakness of MAP estimators.

For all the data sets, the mixed-membership model offers better
                  results. Better on the goodness-of-fit measures and
                  more interpretable results.  The differences were
                  clearer on the bigger data sets with larger
                  state-spaces.

Data Structure: only 1 item per model - the sequences (J=1)
	many replications of each item (big R)

They offer some additional comments on computational algorithms and
                  computational time.},
	Author = {Mark Girolami and Ata Kaban},
	Date-Added = {2013-06-25 19:41:00 +0000},
	Date-Modified = {2013-06-25 19:41:00 +0000},
	Journal = {Data Mining and Knowledge Discovery},
	Keywords = {Latent Dirichlet Allocation, markov chains,
                  Mixture models, user profiling, mixed membership,
                  variational approximation},
	Pages = {175-196},
	Title = {Sequential Activity Profiling: Latent Dirichlet
                  Allocation of Markov Chains},
	Volume = {10},
	Year = {2005},
	Bdsk-File-1 =
                  {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8Qay4uLy4uLy4uL0RvY3VtZW50cy9Kb3VybmFsLU5ld3MtQXJ0aWNsZXMvTWl4ZWQgTWVtYmVyc2hpcC9HaXJvbGFtaSAmIEthYmFuICgyMDA1KS0gTERBIG9mIG1hcmtvdiBjaGFpbnMucGRm0hcLGBlXTlMuZGF0YU8RAlwAAAAAAlwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMtppx9IKwAAAAueSx9HaXJvbGFtaSAmIEthYmFuICgyMDAjQjlFNkEucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC55qyVXqE1BERiAAAAAAAAMABAAACSAAAAAAAAAAAAAAAAAAAAAQTWl4ZWQgTWVtYmVyc2hpcAAQAAgAAMtp7W8AAAARAAgAAMlWMGMAAAABABQAC55LAAucAgAFyJ8ABcieAAC5VwACAHJNYWNpbnRvc2ggSEQ6VXNlcnM6AGFnYWx5YXJkdDoARG9jdW1lbnRzOgBKb3VybmFsLU5ld3MtQXJ0aWNsZXM6AE1peGVkIE1lbWJlcnNoaXA6AEdpcm9sYW1pICYgS2FiYW4gKDIwMCNCOUU2QS5wZGYADgBkADEARwBpAHIAbwBsAGEAbQBpACAAJgAgAEsAYQBiAGEAbgAgACgAMgAwADAANQApAC0AIABMAEQAQQAgAG8AZgAgAG0AYQByAGsAbwB2ACAAYwBoAGEAaQBuAHMALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAHJVc2Vycy9hZ2FseWFyZHQvRG9jdW1lbnRzL0pvdXJuYWwtTmV3cy1BcnRpY2xlcy9NaXhlZCBNZW1iZXJzaGlwL0dpcm9sYW1pICYgS2FiYW4gKDIwMDUpLSBMREEgb2YgbWFya292IGNoYWlucy5wZGYAEwABLwAAFQACABD//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgD8AQEBCQNpA2sDcAN7A4QDkgOWA50DpgOrA7gDuwPNA9AD1QAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAPX}}
